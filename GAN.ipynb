{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrgyvr0I55nD"
      },
      "source": [
        "### **Data Preprocessing and Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evYnnfl9546A"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/diabetes.csv\")\n",
        "print(data)\n",
        "\n",
        "def _df(data):\n",
        "    df = pd.DataFrame(data)\n",
        "    for c in range(df.shape[1]):\n",
        "        mapping = {df.columns[c]: c}\n",
        "        df = df.rename(columns=mapping)\n",
        "    return df\n",
        "\n",
        "# Preprocess data\n",
        "X = data.drop(columns=[\"Outcome\"]).values\n",
        "y = data[\"Outcome\"].values\n",
        "\n",
        "X = KNNImputer().fit_transform(X)\n",
        "data = pd.DataFrame(StandardScaler().fit_transform(np.column_stack((X, y))))\n",
        "\n",
        "# Binarize the \"Outcome\" column in the original data\n",
        "original_outcome = np.where(data.iloc[:, -1] > 0, 1, 0)\n",
        "\n",
        "# Split the original data into train and test sets\n",
        "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(data.drop(columns=[data.columns[-1]]), original_outcome, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a model on the training set of the original data\n",
        "clf_orig = LogisticRegression()\n",
        "clf_orig.fit(X_train_orig, y_train_orig)\n",
        "\n",
        "class Gan():\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.n_epochs = 200\n",
        "\n",
        "    # Noise generation function\n",
        "    def _noise(self):\n",
        "        noise = np.random.normal(0, 1, self.data.shape)\n",
        "        return noise\n",
        "\n",
        "    # Noise generation function\n",
        "\n",
        "    def _noise_with_seed(self, seed):\n",
        "        np.random.seed(seed)\n",
        "        noise = np.random.normal(0, 1, self.data.shape)\n",
        "        return noise\n",
        "\n",
        "\n",
        "    # Generator model architecture\n",
        "    def _generator(self):\n",
        "        model = tf.keras.Sequential(name=\"Generator_model\")\n",
        "        model.add(tf.keras.layers.Dense(15, activation='relu',\n",
        "                                         kernel_initializer='he_uniform',\n",
        "                                         input_dim=self.data.shape[1]))\n",
        "        model.add(tf.keras.layers.Dense(30, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dense(self.data.shape[1], activation='linear'))\n",
        "        return model\n",
        "\n",
        "    # Discriminator model architecture\n",
        "    def _discriminator(self):\n",
        "        model = tf.keras.Sequential(name=\"Discriminator_model\")\n",
        "        model.add(tf.keras.layers.Dense(25, activation='relu',\n",
        "                                         kernel_initializer='he_uniform',\n",
        "                                         input_dim=self.data.shape[1]))\n",
        "        model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    # GAN model architecture\n",
        "    def _GAN(self, generator, discriminator):\n",
        "        discriminator.trainable = False\n",
        "        generator.trainable = True\n",
        "        model = tf.keras.Sequential(name=\"GAN\")\n",
        "        model.add(generator)\n",
        "        model.add(discriminator)\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "        return model\n",
        "\n",
        "    # Training function\n",
        "    def train(self, generator, discriminator, gan):\n",
        "        for epoch in range(self.n_epochs):\n",
        "            generated_data = generator.predict(self._noise())\n",
        "            labels = np.concatenate([np.ones(self.data.shape[0]), np.zeros(self.data.shape[0])])\n",
        "            X = np.concatenate([self.data, generated_data])\n",
        "            discriminator.trainable = True\n",
        "            d_loss, _ = discriminator.train_on_batch(X, labels)\n",
        "\n",
        "            noise = self._noise()\n",
        "            g_loss = gan.train_on_batch(noise, np.ones(self.data.shape[0]))\n",
        "\n",
        "            print('>%d, d1=%.3f, d2=%.3f' % (epoch + 1, d_loss, g_loss))\n",
        "\n",
        "        return generator\n",
        "\n",
        "    # Data generation function\n",
        "    def generate_data(self, generator, num_samples):\n",
        "        noise = self._noise()\n",
        "        generated_data = generator.predict(noise)\n",
        "        return generated_data[:num_samples, :]\n",
        "\n",
        "      # Data generation function with seed\n",
        "    def generate_data_with_seed(self, generator, num_samples, seed=1234):\n",
        "        noise = self._noise_with_seed(seed=seed)\n",
        "        generated_data = generator.predict(noise)\n",
        "        return generated_data[:num_samples, :]\n",
        "\n",
        "    def compute_identifiability(self, original_data, generated_data, weights):\n",
        "        distances = []\n",
        "        for xi in original_data:\n",
        "            min_distance = np.inf\n",
        "            for xj in original_data:\n",
        "                if not np.array_equal(xi, xj):\n",
        "                    distance = np.linalg.norm((xi - xj) * weights)\n",
        "                    min_distance = min(min_distance, distance)\n",
        "            distances.append(min_distance)\n",
        "\n",
        "        synthetic_distances = []\n",
        "        for xi in original_data:\n",
        "            min_distance = np.inf\n",
        "            for xj in generated_data:\n",
        "                distance = np.linalg.norm((xi - xj) * weights)\n",
        "                min_distance = min(min_distance, distance)\n",
        "            synthetic_distances.append(min_distance)\n",
        "\n",
        "        num_identifiable = sum(1 for s, o in zip(synthetic_distances, distances) if s < o)\n",
        "        identifiability = num_identifiable / len(original_data)\n",
        "\n",
        "        return identifiability\n",
        "\n",
        "\n",
        "# Instantiate GAN class and train the model\n",
        "model = Gan(data=data)\n",
        "generator = model._generator()\n",
        "discriminator = model._discriminator()\n",
        "gan_model = model._GAN(generator=generator, discriminator=discriminator)\n",
        "trained_model = model.train(generator=generator, discriminator=discriminator, gan=gan_model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXOVECehupBF"
      },
      "outputs": [],
      "source": [
        "df_last=data.iloc[ :, -1]\n",
        "sum(df_last>=0)/len(df_last)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset Generation**\n"
      ],
      "metadata": {
        "id": "pccQic9VXSR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define different sizes of datasets to be generated\n",
        "dataset_sizes = [2 ** i for i in range(4, 11)]\n",
        "\n",
        "identifiability_scores = {size: [] for size in dataset_sizes}\n",
        "num_seeds = 20  # Number of seeds to use for each dataset size\n",
        "test_accuracy_diffs = []\n",
        "weights = np.array([2, 2, 1, 1, 1, 1, 1, 1, 1])\n",
        "\n",
        "for size in dataset_sizes:\n",
        "    test_accuracies_orig = []\n",
        "    test_accuracies_gen = []\n",
        "    for seed in range(num_seeds):\n",
        "      flag = True\n",
        "      while flag:\n",
        "        # Generate new data of the specified size\n",
        "        new_data = model.generate_data_with_seed(generator, size, seed=seed)\n",
        "        # Binarize the \"Outcome\" column in the generated data\n",
        "        generated_outcome = np.where(new_data[:, -1] > 0, 1, 0)\n",
        "        if sum(generated_outcome)>0 and sum(generated_outcome)<len(generated_outcome) :\n",
        "          flag = False\n",
        "        else:\n",
        "          seed = seed + 100\n",
        "\n",
        "      # Train a model on the generated data\n",
        "      clf_gen = LogisticRegression()\n",
        "      clf_gen.fit(new_data[:, :-1], generated_outcome)\n",
        "\n",
        "\n",
        "      # Test the two models on the test set of the original data\n",
        "      accuracy_orig = accuracy_score(y_test_orig, clf_orig.predict(X_test_orig))\n",
        "      accuracy_gen = accuracy_score(y_test_orig, clf_gen.predict(X_test_orig))\n",
        "\n",
        "      # Compute the difference in test accuracies\n",
        "      difference_accuracy = abs(accuracy_orig - accuracy_gen)\n",
        "\n",
        "      test_accuracies_orig.append(accuracy_orig)\n",
        "      test_accuracies_gen.append(accuracy_gen)\n",
        "\n",
        "      # Compute identifiability\n",
        "      identifiability_measure = model.compute_identifiability(data.values, new_data, weights)\n",
        "      identifiability_scores[size].append(identifiability_measure)\n",
        "      print(f\"Seed {seed + 1}: ε-Identifiability Score: {identifiability_measure}\")\n",
        "\n",
        "    diff_accuracy = (abs(np.array(test_accuracies_orig) - np.array(test_accuracies_gen)))\n",
        "\n",
        "    test_accuracy_diffs.append(diff_accuracy)\n",
        "\n",
        "    print(f\"Dataset size: {size}, Difference in test accuracies: {diff_accuracy}\")\n"
      ],
      "metadata": {
        "id": "f6C3Ly9uWkDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evalution of the quality**"
      ],
      "metadata": {
        "id": "jj1KetkkX2n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the variations of the quality with different dataset sizes using box plots\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.boxplot(test_accuracy_diffs, labels=dataset_sizes)\n",
        "plt.title('Variations in Quality with Different Dataset Sizes')\n",
        "plt.xlabel('Dataset Size')\n",
        "plt.ylabel('Difference in Test Accuracies')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p6v5ve5JXrwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation of the privacy**"
      ],
      "metadata": {
        "id": "0l4CPQeqYDIh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg9VJkBLiBbY"
      },
      "outputs": [],
      "source": [
        "# Plot the variations of identifiability with different dataset sizes using box plots\n",
        "identifiability_scores_list = [scores for _, scores in identifiability_scores.items()]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.boxplot(identifiability_scores_list, labels=dataset_sizes)\n",
        "plt.title('Variations in Privacy with Different Dataset Sizes')\n",
        "plt.xlabel('Dataset Size')\n",
        "plt.ylabel('ε-Identifiability Score')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Quality Vs Privacy**"
      ],
      "metadata": {
        "id": "v2LdpscUYKgS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_DDbn9_rIIF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming identifiability_scores and test_accuracy_diffs are already computed\n",
        "\n",
        "# Define marker styles for each dataset size\n",
        "marker_styles = ['o', 's', '^', 'D', 'v', '>', '<', 'p']\n",
        "\n",
        "# Plot the variations of quality vs. identifiability scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, size in enumerate(dataset_sizes):\n",
        "    plt.scatter(identifiability_scores[size], test_accuracy_diffs[i], label=f'Dataset Size: {size}', marker=marker_styles[i])\n",
        "plt.title('Variations of Quality vs. Identifiability Scores')\n",
        "plt.xlabel('ε-Identifiability Score')\n",
        "plt.ylabel('Difference in Test Accuracies')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Distribution of the generated data**"
      ],
      "metadata": {
        "id": "HxvRV7KbYS03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
        "ax[0].scatter(data.iloc[:, 0], data.iloc[:, 1])\n",
        "ax[1].scatter(new_data[:, 0], new_data[:, 1])\n",
        "ax[0].set_title(\"Original Data\")\n",
        "ax[1].set_title(\"synthetic Data\")"
      ],
      "metadata": {
        "id": "C-ziwkduy7wh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}